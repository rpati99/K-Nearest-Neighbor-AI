{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports to run this notebook.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score  # To measure the accuracy score for outputting the metric in the console\n",
    "from sklearn.metrics.pairwise import euclidean_distances  # To compute Euclidean distances \n",
    "from sklearn.model_selection import LeaveOneOut # Using leave one out cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "def load_dataset(file_path):\n",
    "    data = np.loadtxt(file_path)\n",
    "    X = data[:, 1:]  # Capturing data from 2nd column since 1st column is containing labels y\n",
    "    y = data[:, 0].astype(int)  # Assigning labels \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemented this KNearestNeighbors Classifier logic based on this youtube tutorial below:- \n",
    "# https://www.youtube.com/watch?v=rTEtEy5o3X0\n",
    "class KNearestNeighbors:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for x_test in X_test:\n",
    "            # Compute the Euclidean distances from x_test to all training samples\n",
    "            distances = euclidean_distances(x_test.reshape(1, -1), self.X_train).flatten()\n",
    "            \n",
    "            # Get the indices of the k nearest neighbors\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            \n",
    "            # Extract the labels of the k nearest neighbors\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "            \n",
    "            # Use the most common label from the majority vote\n",
    "            most_common = Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "            \n",
    "            predictions.append(most_common)\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code referenced from the MATLAB code in slide \"Project_2_Briefing.pdf\"\n",
    "def leave_one_out_cross_validation(X, y, feature_indices):\n",
    "    # Initialize the K-Nearest Neighbors classifier with k=3 for LOOCV\n",
    "    classifier = KNearestNeighbors(k=3)\n",
    "    \n",
    "    # Prepare the dataset with only the selected features\n",
    "    X_subset = X[:, feature_indices]\n",
    "    \n",
    "    # Set up the leave-one-out cross-validation\n",
    "    loo = LeaveOneOut()\n",
    "    number_correctly_classified = 0\n",
    "    \n",
    "    # Iterate over each fold for leave-one-out\n",
    "    for train_index, test_index in loo.split(X_subset):\n",
    "        X_train, X_test = X_subset[train_index], X_subset[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Fit the model on the training part\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict the outcome for the test part\n",
    "        predicted_label = classifier.predict(X_test)\n",
    "        \n",
    "        # Count correct predictions\n",
    "        if predicted_label == y_test:\n",
    "            number_correctly_classified += 1\n",
    "    \n",
    "    # Calculate and return accuracy as the ratio of correctly classified instances\n",
    "    accuracy = number_correctly_classified / len(X_subset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X, y, method='forward'):\n",
    "    n_features = X.shape[1] # fetching the columns amount \n",
    "    if method == 'forward': # Code logic referenced from Slide \"Project_2_Briefing.pdf\" where it was used to implement the logic\n",
    "        \n",
    "        # Starting with no features selected \n",
    "        current_features = []\n",
    "        best_accuracy = 0\n",
    "        \n",
    "        while len(current_features) < n_features:\n",
    "            best_current_feature = None\n",
    "            \n",
    "            # Adds the each feature to the current set and checks for accuracy\n",
    "            for feature in range(n_features):\n",
    "                \n",
    "                if feature in current_features:\n",
    "                    continue # Skipping if feature is already selected \n",
    "                \n",
    "                # Temporarily adding features and selecting accuracy \n",
    "                temp_features = current_features + [feature] \n",
    "                accuracy = leave_one_out_cross_validation(X, y, temp_features)\n",
    "                print(f\"Using feature(s) {[ f + 1 for f in temp_features ]} accuracy is {accuracy*100:.1f}%\")\n",
    "                \n",
    "                # Remembering the feature if it is improving accuracy \n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_current_feature = feature\n",
    "                    \n",
    "            # Adding the feature to current set if it is improving accuracy \n",
    "            if best_current_feature is not None:\n",
    "                current_features.append(best_current_feature)\n",
    "                print(f\"Feature set {[ f + 1 for f in current_features]} was best, accuracy is {best_accuracy*100:.1f}%\")\n",
    "            else:\n",
    "                break # If no feature is improving the accuracy, we stop the search \n",
    "            \n",
    "        print(f\"Finished search!! The best feature subset is {[ f + 1 for f in current_features]}, which has an accuracy of {best_accuracy*100:.1f}%\")\n",
    "        return current_features\n",
    "\n",
    "    elif method == 'backward':\n",
    "        \n",
    "        # Starting with all features selected \n",
    "        current_features = list(range(n_features)) # I used forward elimination's code which was derived from  Slide \"Project_2_Briefing.pdf\" and then built upon it\n",
    "        while len(current_features) > 1:\n",
    "            worst_current_feature = None\n",
    "            best_current_accuracy = 0\n",
    "            \n",
    "            # Removes each feature to the current set and checks for accuracy\n",
    "            for feature in current_features:\n",
    "                temp_features = current_features[:]\n",
    "                temp_features.remove(feature)\n",
    "                accuracy = leave_one_out_cross_validation(X, y, temp_features)\n",
    "                print(f\"Using feature(s) {[ f + 1 for f in temp_features]} accuracy is {accuracy*100:.1f}%\")\n",
    "                \n",
    "                # If removal of the feature improves accuracy then we are remembering this feature \n",
    "                if accuracy > best_current_accuracy:\n",
    "                    best_current_accuracy = accuracy\n",
    "                    worst_current_feature = feature\n",
    "            \n",
    "            # Finding the feature whose removal improves accuracy, we remove it from the current set\n",
    "            if worst_current_feature is not None:\n",
    "                current_features.remove(worst_current_feature)\n",
    "                print(f\"Feature set {[ f + 1 for f in current_features]} was best, accuracy is {best_current_accuracy*100:.1f}%\")\n",
    "            else:\n",
    "                break # If there was no improvement in accuracy by removing the feature, we stop the search \n",
    "        print(f\"Finished search!! The best feature subset is {[ f + 1 for f in current_features]}, which has an accuracy of {best_current_accuracy*100:.1f}%\")\n",
    "        return current_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = input(\"Welcome to Rachit Prajapati's Feature Selection Algorithm.\\nType in the name of the file to test: \") \n",
    "X, y = load_dataset(file_name) # Fetching the file by path and extracting it by data in columns and labels \n",
    "print(f\"This dataset has {X.shape[1]} features (not including the class attribute), with {X.shape[0]} instances.\")\n",
    "\n",
    "method = input(\"Type the number of the algorithm you want to run.\\n1) Forward Selection\\n2) Backward Elimination\\n\")\n",
    "method = 'forward' if method == '1' else 'backward' # code to select the respective mode \n",
    "\n",
    "selected_features = feature_selection(X, y, method) # Call the search algorithm as per the respective mode selected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
